#+title: etcd
#+roam_tags: self-host service-configuration
* info

  redhat -> coreos -> etcd

  [[https://etcd.io/][doc]]
  [[https://github.com/etcd-io][git]]

  3.5的etcd和之前的版本有很大的不同，主要是因为go mod导致的文件路径改动
  
* 把玩一下

** 编译安装
   make即make build,即./build本地的一个shell脚本

   编译出来两个可执行文件
   - ./bin/etcd :: 是拿来跑的server，单机也可以启动
   - ./bin/etcdctl :: 是相当于client类似功能的东西，更主要是控制
  
** 单节点
   ./bin/etcd 其实可以直接起，默认端口2379

   ./bin/etcdctl --endpoints=etcd的地址 put/get/lease/watch/lock 等等功能
   
* 基本用法
  
** 服务发现
   默认前缀 _etcd/registry

** kv
   - put/get/del
   - --from-key :: 从开始
   - --prefix :: 前缀
   - --rev :: 第n个历史版本
  
** watch
   查看key变化情况
   
** lease
   lease grant 60得到一个hash,附在--lease后面可以共享ttl

   lease keep-alive 这个hash可以续

   lease timetolive 查看ttl
   
* 文件结构
  比较乱，没什么层次

  - api :: 不知道干嘛的
  - bin :: 打出来的二进制放的地方，运行makefile才会有
  - client :: 放客户端，v2和v3对应的是不兼容的服务器版本，两者不是同一种功能的提升
  - contrib :: 一些工具的配置文件，运维用的
  - default.etcd :: 初始数据的例子
  - Documentation :: markdown文档
  - etcdctl :: etcdctl的逻辑
  - hack :: 一些工具配置文件，开发用的
  - pkg :: 一些逻辑
  - raft :: raft
  - scripts :: 开发脚本
  - security :: 一些文档，没啥用
  - server :: 服务器逻辑
  - tests :: 针对二进制功能的测试文件
  - tools :: 一些脚本工具，类似于mysqldump
  
** 入口文件
   服务器主文件在server/main.go，其他服务器实现也在server/下

   ctl主文件在etcdctl/main.go

   当然我们主要用的是server
** 基本服务器实现

*** log
    默认使用zap，文档里的capnslog已经deprecated了

*** monitor
    在/metrics里面

*** cmd
    就两个转发的command使用cobra，tcp级别的转发和grpc级别的转发
    
** 逻辑分发::server/etcdmain/etcd.go:startEtcdOrProxyV2
   常规操作，初始化config和log

   判断运行模式，有server和proxy两种

   先从存储的文件名判断一下，这两种模式的文件夹名不同，如果已有文件检查到之前运行的是哪个就这次也运行哪个

   然后看是不是server满了，如果满了，而且一个fallback的选项打开了之后，就还是proxy，没打开就退出了

   主要我们用的还是server，也主要看这个

   进入到下面的StartEtcd，这里面又是一个初始化，但是不会亲自跑逻辑循环，所以会很快退出来，它后续的代码，首先检查下这个的结果，然后卡住进程不让main退出，再打点日志。
   
*** config
    初始化config在server/config.go里
    flag用的是标准库的flag，continueonerror
    
** 主逻辑::server/embed/etcd.go:StartEtcd
   从startEtcd封装了一层收尸，马上到server/embed/etcd.go

   一开头调用外部Config的validate方法，这个方法干的事情很多
   - 确认log输出
     - 输出路径，里面有一个/dev/null就全部不输出
     - 日志等级
     - 是否用journal
     - 记录grpclog的日志
     - 记录tls的日志
   - 几个url只能是ip，Unix路径和localhost，但是hosturl可以是域名
   - 还有一些raft和存储的配置，比如不能等于0，大于几倍这种

   之后读取之前的文件，data文件夹这种

   有自己的config方法，不过都是之前引过来重新组合的

   start方法是运行

   有peer, client, monitor三个主要服务，
   - peer处理来自其他etcd的请求，是一个包装成http的grpc服务
   - client处理用户请求，也是同样的包装
   - monitor搞一些监控内容

   peer和client会configure，peer是数组，client是键为url的map
   peer主要用于创造自签名的证书对，一堆加密函数
   client除了上面的还有监听端口套接字之类的配置

   查看是否初始化过，使用配置的urls和token

   重新搞个config，放到NewServer函数中

   NewServer函数是个初始化Etcd对象的函数，在server/v3/etcdserver里，先不管这个了

   接下来会计算一下本地kv的hash，用来看跟远程是不是一样，会指定版本
   然后调用Start方法，这个方法比之前的NewServer简单点，但是在一个包里，待会一起看

   最后servePeers，serveClients，serveMetrics，这三个巨复杂，一会再看
   
** newserver
   先新建了一个内存里的store对象，这个对象好复杂，有很多node,NodeExtern什么的东西，初始化倒挺容易的，新建了一个根node，node就是那种可以getset的节点，所以根是文件夹，然后下面两个子node，cluster的/0和key的/1，理所当然的这个对象还有watch的信息watchhub和过期的信息ttlheap。

   然后检查本地的文件夹，比如datadir,它拼了一些地址，比如$datadir/member/wal,$datadir/member/snap,$snapdir/db

   顺便恢复snap，打开或新建bbolt的后端存储

   初始化peer的transport,也叫roundtripper,就是处理主动的http一次性request和response的

   恢复持久化信息，包括本地信息和集群信息，主要看之前的haswal和newcluster，他代码的顺序有点怪，先看haswal，然后没有再看newcluster比较正常

   如果有haswal，从wal文件夹里找最近的snap恢复,会先检查snap，检查很多东西，都是pb里的，现在看不懂,然后找最新的，回复到后端db里，启动节点，同步到集群或者单点模式

   如果不是haswal但是要加入一个集群，会检查集群有效性，通过一个v2的接口，此时那个老集群要打开v2的api *ETCD_ENABLE_V2=true*，验证一下url是否一致，验证一下版本是否兼容，比如现在的etcd版本是3.5-alpha0，那新起的版本可以加入3.0到3.5-alpha0之前的集群，最后赋值一些id，存储对象，存储后端，启动startNode本地节点，然后赋值本地id。发现startNode也挺复杂的。

   如果不是has，而且要新建一个集群。先看看本地在不在集群里，然后看bootstrap条件，主要是peer和advertises是不是一样的。
   
   etcd的一个clusterid是组内ip哈希的，所以同样ip的集群，id就不会变。
 
* etcd实现
 主要是实现可以服务发现的接口
  
** 接口
   提供http和protobuf的接口

** raft

** 存储

